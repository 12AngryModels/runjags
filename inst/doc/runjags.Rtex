\documentclass[nojss]{jss}

\usepackage[utf8]{inputenc} 
%\VignetteIndexEntry{Using the runjags package}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Matthew J Denwood\\University of Copenhagen}
\title{\pkg{runjags}:  An \proglang{R} Package Providing Interface Utilities, Distributed Computing Methods and Additional Distributions For MCMC Models in \proglang{JAGS}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Matthew J Denwood} %% comma-separated
\Plaintitle{runjags:  An R Package Providing Interface Utilities, Distributed Computing Methods and Additional Distributions For MCMC Models in JAGS} %% without formatting
\Shorttitle{\pkg{runjags}: \proglang{JAGS} interface utilities and additional distributions} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  The \pkg{runjags} package provides a set of interface functions to facilitate running Markov chain Monte Carlo models in \proglang{JAGS} from within \proglang{R}.  Automated calculation of appropriate convergence and sample length diagnostics, user-friendly access to commonly used graphical outputs and summary statistics, and parallelised methods of running \proglang{JAGS} are provided.  The primary motivation is to provide automated methods of analysis of simulated data to facilitate model performance assessment and drop-$k$ type cross-validation studies using high performance computing clusters provided by \pkg{snow}.  A module extension for \proglang{JAGS} providing the Pareto family of distributions is also included within \pkg{runjags}. This vignette is taken from the publication for the \pkg{runjags} package \citep{Denwood:2014ee}.  It outlines the primary functions of this package, and gives an illustration of a simulation study to assess the sensitivity of a gamma-Poisson (negative binomial) distribution to three different ‘minimally informative’ priors.  
}
\Keywords{MCMC, Bayesian, graphical models, interface utilities, \proglang{JAGS}, \pkg{BUGS}, \proglang{R}}
\Plainkeywords{MCMC, Bayesian, graphical models, interface utilities, JAGS, BUGS, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Matthew J Denwood\\
  Section for Animal Welfare and Disease Control\\
  Faculty of Health and Medical Sciences\\
  University of Copenhagen\\
  Denmark\\
  E-mail: \email{md@sund.ku.dk}\\
  URL: \url{http://iph.ku.dk/english/employees/?pure=en/persons/487288/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

\usepackage{amsmath}
\usepackage{amssymb}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\jagscode}[1]{{\tt #1}}
% or:
%\newcommand{\jagscode}[1]{$#1$}



\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.


\section{Introduction}

Over the last decade, the increase in availability of computing power has led to a substantial increase in the availability and use of computationally intensive statistical methods;  amongst the most widely adopted of these are Bayesian Markov chain Monte Carlo (MCMC) methods \citep{Gilks:1998pt}.  However, such methods have potential drawbacks if used inappropriately, including difficulties identifying convergence \citep{Toft:2007vl, Brooks:1998ee} and the potential for autocorrelation to decrease the effective sample size of the numerical integration process \citep{Kass:1998fu}.  

Although writing customised MCMC sampling algorithms is relatively straightforward, particularly using the Metropolis-Hastings algorithm \citep{Hastings:1970bi}, it has become more common practice to employ more general Bayesian MCMC fitting software such as the Bayesian analysis Using Gibbs Sampling (\pkg{BUGS}) software variants \pkg{WinBUGS} and \pkg{OpenBUGS} \citep{Lunn00}.  One alternative known as Just Another Gibbs Sampler (\proglang{JAGS}) has more recently been made available by \cite{jags}, and offers cross-platform support, a direct interface to \proglang{R} using \pkg{rjags} \citep{Plummer2013}, as well as being extendable with user specified modules written in \proglang{C++} to add support for additional distributions and random number generators \citep{Wabersich}.  Both \proglang{JAGS} and \pkg{WinBUGS}/\pkg{OpenBUGS} use the \proglang{BUGS} syntax to allow arbitrary models to be more easily defined by the user, and provide sufficient flexibility to be used for the vast majority of biological statistical problems.  This flexibility and ease of use makes using the \proglang{BUGS} language attractive and attainable for researchers who may be somewhat familiar with more traditional frequentist modelling techniques, but are not aware of the potential issues with MCMC analysis, hence the prominent warning that `MCMC sampling can be dangerous' in the \pkg{WinBUGS} user manual \citep{Lunn00}.  One way to reduce some of this potential risk for inexperienced users would be to provide a wrapper for the model fitting software that analyses the model output for common problems, such as failure to converge, parameter auto-correlation and effective sample size, that may otherwise be missed by the end user.  

Besides the potential problems associated with convergence and auto-correlation, one of the most commonly criticised aspects of Bayesian MCMC is the requirement for prior belief to be incorporated into the model.  This is of course also a potentially huge advantage of Bayesian methods, and many methods exist for the elicitation of prior distributions to improve inference \citep{Garthwaite2005}, but the perceived lack of objectivity of informative priors leads some authors to advocate minimally informative priors \citep{Kass1996}.  There are a number of different recommendations for an appropriate choice of prior distribution in various different circumstances, for example the half-Cauchy distribution has been recommended as a reasonable choice for standard deviation parameters within hierarchical models \citep{Gelman2006,Polson2011}, and \citet{DuMouchel1994} gives an argument for the use of $\pi(\tau) = \frac{s_0}{\left(s_0 + \tau\right)^2}$ as a prior for a variance parameter $\tau$ in meta-analysis models.  However, there are frequently several minimally informative priors that could be equally justifiable in a given situation, and choice between these is known to affect the shape of the posterior \citep{Lele2009}, particularly when the information in the data is relatively sparse.  For example, \cite{Tuyl2008} reviewed the options for specifying a prior for Binomial events, and found that the Jeffreys prior in this situation performed poorly.  The choice of prior distribution can also adversely affect model convergence and identifiability; particularly the \jagscode{Gamma(0.001,0.001)} prior, which is often used for precision parameters but is almost improper.  In addition, the flexibility of MCMC techniques and often complex structure of the data can lead to situations where multiple syntactically different but conceptually equivalent models could be devised, which also has the potential to affect the inference made.  Validation of model formulation and prior selection using simulated data is a pragmatic way of guarding against these problems, but is rarely performed perhaps due to the computational effort required, and the requirement to assess the convergence and effective sample size of repeated MCMC simulations.  Although assessment of these factors should always be performed manually in real-world situations, a method of automating these procedures may be warranted for analysis of simulated data to assess model performance.

This paper describes the \pkg{runjags} package for \proglang{R} which can be used to automate MCMC fitting and summarisation procedures for \proglang{JAGS} models, particularly for a simulation study with an arbitrary number of replicate datasets, and also provides additional distributions to extend the core functionality of \proglang{JAGS}.  An example of usage to assess the sensitivity of an over-dispersed count observation model to various minimally informative prior distributions is also provided.  Some prior familiarity with the \proglang{BUGS} programming language and the underlying MCMC algorithms is assumed.

\section{Package functions}

\subsection{Basic usage}

The core functionality of the runjags package allows a model specified by the user to be run using a variety of methods to call \proglang{JAGS}.  This model can be specified in an external text file, which is likely to be preferable for more complex model formulations, or as a character string within \proglang{R}, which eliminates the need for multiple text files.  External text files containing data and initial value lists compatible with \pkg{WinBUGS} are also supported, and will be converted to the required \proglang{JAGS} format (more details are given in the read.winbugs help page).

The model formulation can also contain three special inline statements which are interpreted by the runjags package:  \jagscode{\#data\#}, which indicates that the comma separated variable names to the right of the statement are to be included in the simulation as data, and similarly \jagscode{\#monitor\#} which indicates variable names to monitor, and \jagscode{\#inits\#} which indicates variables for which initial values are to be provided.  Variables specified by \jagscode{\#data\#} and \jagscode{\#inits\#} will be automatically retrieved from a named list provided using the `datalist' and `initlist' arguments, or failing that from the parent environment for the function call.  For initial values, a list of length equal to the number of chains containing initial values to be used for each chain should be provided; if only a single set of initial values is provided a warning will be given.  The \jagscode{\#data\#} and \jagscode{\#inits\#} variable names may also match a function returning an appropriate vector, in the case of initial values this function may accept a single argument indicating the chain that the initial values are to be used for.  Multiple inline statements can be used, and will simply be combined.

There are several options to the `run.jags' function including explicit specification of monitors, data and initial values, the required burn in period, sampling length and thinning interval, the summary statistics to calculate from the chains, the method to use for calling the \proglang{JAGS} executable, and options for `modules' and `factories' which allow a character vector of \proglang{JAGS} extension modules and factories to be loaded.  A basic model run with a fixed burn in period (default 4000 iterations after 1000 adaptive iterations) and sampling period (default 10000 iterations) can be obtained as follows.

Specify a JAGS model as a character vector:
\begin{Schunk}
\begin{Sinput}
R> model <- "model {
+	for(i in 1 : N){ #data# N
+	Y[i] ~ dnorm(true.y[i], precision) #data# Y
+	true.y[i] <- (coef * X[i]) + int #data# X
+  }
+  coef ~ dunif(-1000,1000)
+  int ~ dunif(-1000,1000)
+  precision ~ dexp(1)
+  #inits# coef, int, precision, .RNG.seed, .RNG.name
+  #monitor# coef, int, precision
+  }"
\end{Sinput}
\end{Schunk}
Simulate the data:
\begin{Schunk}
\begin{Sinput}
R> set.seed(1)
R> X <- 1:100
R> Y <- rnorm(length(X), 2*X + 10, 1)
R> N <- length(X)
\end{Sinput}
\end{Schunk}
A function to return initial values (including RNG seeds) for each chain:
\begin{Schunk}
\begin{Sinput}
R> coef <- function(chain)
+  return( switch(chain, "1"= -10, "2"= 10) )
R> int <- function(chain) 
+  return( switch(chain, "1"= -10, "2"= 10) )
R> precision <- function(chain)
+  return( switch(chain, "1"= 0.01, "2"= 100) )
R> .RNG.seed <- function(chain) 
+  return( switch(chain, "1"= 1, "2"= 2) )
R> .RNG.name <- function(chain)
+  return( switch(chain, "1" = "base::Super-Duper",
+  "2" = "base::Wichmann-Hill") )
\end{Sinput}
\end{Schunk}
Run the simulation:
\begin{Schunk}
\begin{Sinput}
R> results <- run.jags(model, n.chains = 2, method="interruptible")
\end{Sinput}
\begin{Soutput}
Calling the simulation...
Welcome to JAGS 3.4.0 on Mon Nov 18 21:37:47 2013
JAGS is free software and comes with ABSOLUTELY NO WARRANTY
Loading module: basemod: ok
Loading module: bugs: ok
. . Reading data file data.txt
. Compiling model graph
   Resolving undeclared variables
   Allocating nodes
   Graph Size: 407
. Reading parameter file inits1.txt
. Reading parameter file inits2.txt
. Initializing model
. Adapting 1000
-------------------------------------------------| 1000
++++++++++++++++++++++++++++++++++++++++++++++++++ 100%
Adaptation successful
. Updating 4000
-------------------------------------------------| 4000
************************************************** 100%
. . . . Updating 10000
-------------------------------------------------| 10000
************************************************** 100%
. . . . Updating 0
. Deleting model
. 
Simulation complete.  Reading coda files...
Coda files loaded successfully
Calculating the Gelman-Rubin statistic for 3 variables....
The Gelman-Rubin statistic is below 1.05 for all parameters
Finished running the simulation
\end{Soutput}
\end{Schunk}

The simple model shown (a linear model with intercept and single explanatory variable) is run using two chains, contains three variables that are monitored, and convergence is assessed using the Gelman-Rubin statistic \citep{Gelman1992}.  The method for running the model shown here does so by calling an external \proglang{JAGS} process in the foreground, but a variety of methods are available:  `parallel' runs separate \proglang{JAGS} processes for parallel chains, `background' and `bgparallel' run the model in the background with control of the \proglang{R} interface returning to the user while it is running, `snow' runs parallel chains in separate \proglang{JAGS} processes using a (possibly user-specified) distributed computing cluster, `rjags' uses the \pkg{rjags} package to run \proglang{JAGS}, and `rjparallel' runs parallel chains using separate \pkg{rjags} models using a distributed computing cluster.  The parallel methods (`parallel', `bgparallel' and `rjparallel') should speed up computation of models with multiple chains on multi-core machines, and all issues pertaining to pseudo-random number generation are automatically handled by \pkg{runjags}:  in this example the RNG type and seed is set for each chain, but if none is provided each chain will automatically be given a different random number generator in order to avoid possible issues with non-independence between chains.  The method used for each model run can be controlled directly using the `method' argument, or by changing the default using the `runjags.options' function to permanently set an alternative method as the default.  For example the following code will allow a possibly lengthy \proglang{JAGS} simulation to be run in the background using two processors to speed up the simulation:

\begin{Schunk}
\begin{Sinput}
R> info <- run.jags(model, n.chains = 2, method = 'bgparallel')
\end{Sinput}
\begin{Soutput}
Starting the simulation in the background...
The JAGS process is now running in the background
\end{Soutput}
\end{Schunk}
This returns control of the terminal to the user, who can then carry on working in R while waiting for the simulation to complete, then retrieve the results once it has:
\begin{Schunk}
\begin{Sinput}
R> results <- results.jags(info)
\end{Sinput}
\begin{Soutput}
Simulation complete.  Reading coda files...
Coda files loaded successfully
Calculating the Gelman-Rubin statistic for 3 variables....
The Gelman-Rubin statistic is below 1.05 for all parameters
Finished running the simulation
\end{Soutput}
\end{Schunk}

If the model did not converge or take sufficient samples from the initial \proglang{JAGS} call, the model can be extended (using a different method to call \proglang{JAGS} if desired) for a fixed additional number of samples with the `extend.jags' function, with the initial results either being combined with the new simulation or discarded.  Alternatively, the simulation can be automatically extended using the `autoextend.jags' function, which ensures that the Gelman-Rubin statistic for all monitored parameters meets the specified target to indicate convergence, and that the required number of samples is obtained.  The automated assessment of convergence should always be verified manually for inference to be relied upon, but a fully automated analysis may be sufficient for simulated data.  The initial simulation may also be called using `autorun.jags' in place of `run.jags' to fully automate control of the MCMC simulation run length from the start.

The output of these functions is an object of class `runjags'.  This class is associated with S3 methods for print, plot, as.mcmc and as.mcmc.list - all of which may take a `vars' argument to specify a subset of monitored nodes (using partial matching).  Further utility functions are available for combining multiple objects (`combine.mcmc') and for conversion to/from objects produced by the rjags package (`as.runjags' and `as.jags').  The plot method is intended to be used for convergence diagnostics, and has further arguments for commonly used layout and plot type arguments, but is not intended to be used for producing more specific graphical output from converged MCMC chains for which plot methods associated with `mcmc' or `mcmc.list' objects are more appropriate.  A typical examination of the output of a simulation (the default print method, and a trace plot output for variable names partially matching the letter `c') could be obtained as follows:

\begin{Schunk}
\begin{Sinput}
R> results
\end{Sinput}
\begin{Soutput}

JAGS model summary statistics from 20000 samples 
(chains = 2; burnin = 5000):
                                                 
          Lower95 Median Upper95   Mean        SD
coef       1.9935 1.9995   2.006 1.9996 0.0031642
int        9.7534  10.13   10.48 10.131   0.18577
precision 0.89024 1.2131  1.5533 1.2213   0.17145
                                                    
                MCerr MC%ofSD SSeff     AC.10   psrf
coef      0.000076615     2.4  1706    0.1895 1.0003
int         0.0044427     2.4  1748   0.19662 1.0002
precision   0.0012484     0.7 18862 0.0053645      1

Total time taken: 11.9 seconds
\end{Soutput}
\begin{Sinput}
R> plot(results, type = "trace", vars = "c", layout = c(2,1) )
\end{Sinput}
\begin{Soutput}
Producing 2 plots for 2 variables to the active graphics device
(see ?runjagsclass for options to this S3 method)
\end{Soutput}
\end{Schunk}

\begin{figure}
\begin{center}
\includegraphics{traceplots}
\end{center}
\caption{A traceplot displayed by the plot method for the runjags class, showing only parameters partially matched using the letter `c'.  Multiple chains are shown using different colours.}
\end{figure}

The print method displays information on the median and 95\% credible interval (CI) estimates, as well as the mean, standard deviation, Monte Carlo error, Monte Carlo error as a proportion of sample standard deviation, the effective sample size, autocorrelation at lag 10, and potential scale reduction factor of the Gelman-Rubin statistic \citep{Gelman1992,Brooks1998}.  The plot method displays trace, density and/or cross-correlation plots for the variables specified, using either a single graphics device or by opening multiple devices, or by saving the graphical output to a file for inspection at a later time.  The \pkg{coda} package \citep{Plummer2006} provides many of the underlying functions that calculate these statistics.


\subsection{Simulation studies}

The principle motivation behind development of the \pkg{runjags} package is to automate the analysis of simulated datasets for the purposes of model validation.  While it is possible to repeatedly use the `autorun.jags' function to analyse multiple datasets, a higher level `run.jags.study' function is provided to automate much of this process.  This function takes arguments specifying the number of datasets to analyse, the model to use, a function to produce data that will be provided to each simulation, and a named list of `target' variables with true values representing parameters to be monitored and used to summarise the output of the simulation.  Inline \jagscode{\#inits\#} and \jagscode{\#monitor\#} statements can be used as before, and any target variables are automatically monitored.  Any variables specified using the inline \jagscode{\#data\#} statement will be retrieved from the working environment as usual and will be common to all simulations - data which is intended to change between simulations must therefore be provided using the `data function' argument instead. The `run.jags.study' function can also be used to automate drop-$k$ type validation studies, by specifying a data function that returns the same dataset for each simulation but with a different value (or values) made missing, and the full data as a target variable.  An illustration of this function is provided in Section~\ref{sec:illustration}.

Large simulation studies are likely to be quite computationally intensive, but are an ideal candidate for parallelisation.  For this reason, parallel computation is built directly into the `run.jags.study' function using the \pkg{parallel} package.  This can be used to parallelise the simulation locally, or using a high performance computing cluster set up using the \pkg{snow} package \citep{Tierney2013} and passed to the simulation study using the `cl' argument.  This allows the available computing power to be maximised without requiring any additional code to be written by the end user, including an initial check to ensure that the model compiles and runs locally (using a randomly chosen dataset) before beginning the parallelised study.

\subsection[JAGS module]{\proglang{JAGS} module}
\label{sec:pareto}

Besides the \proglang{R} code specified above, \pkg{runjags} also includes a modular extension to \proglang{JAGS} that provides users with access to an additional suite of functions for the Pareto family of distributions; extending the Pareto Type I distribution provided within \proglang{JAGS} itself to Pareto Types II, III, and IV distributions, as well as the generalised Pareto distribution, the Lomax distribution (a special case of the Pareto Type II distribution with $\mu=0$), and a distribution advocated by \citet{DuMouchel1994} for use with variance parameters (discussed in Section~\ref{sec:priors}).  The usage, PDF and lower bound for the support of each distribution is shown in Table~\ref{tab:pareto} (all distributions have an upper bound of $\infty$ unless otherwise stated).

\begin{table}
\centering

\begin{tabular}{ l l c c  }

\hline
Name & Usage & Density & Lower \\
\hline
Pareto I  $^1$ & \parbox[t]{5.5cm}{\tt dpar1(alpha,sigma)\\ $\alpha>0, \sigma>0$} &  \parbox[c][1.5cm]{5.5cm}{$$\alpha \, \sigma^\alpha \, x^{-\left(\alpha + 1\right)}$$ }& $\sigma$ \\
Pareto II & \parbox[t]{5.5cm}{\tt dpar2(alpha,sigma,mu) \\ $\alpha>0, \sigma>0$ } & \parbox[c][1.5cm]{5.5cm}{$$\frac{\alpha}{\sigma} \left(\frac{\sigma+x-\mu}{\sigma}\right)^{-\left(\alpha + 1\right)}$$} & $\mu$ \\
Pareto III & \parbox[t]{5.5cm}{\tt dpar3(sigma,mu,gamma) \\ $\sigma>0, \gamma>0$ } & \parbox[c][1.5cm]{5.5cm}{$$ \frac{\left(\frac{x-\mu}{\sigma}\right)^{\frac{1}{\gamma} -1}  {\left(\frac{x-\mu}{\sigma}^{\frac{1}{\gamma}} + 1\right)}^{-2}}{\gamma \, \sigma} $$} & $\mu$ \\
Pareto IV & \parbox[t]{5.5cm}{\tt dpar4(alpha,sigma,mu,gamma) \\ $\alpha>0, \sigma>0, \gamma>0$ } & \parbox[c][1.5cm]{5.5cm}{$$ \frac{\alpha  \left(\frac{x-\mu}{\sigma}\right)^{\frac{1}{\gamma} -1}  {\left(\frac{x-\mu}{\sigma}^{\frac{1}{\gamma}} + 1\right)}^{-\left(\alpha+1\right)}}{\gamma \, \sigma} $$} & $\mu$ \\
Lomax  $^2$ & \parbox[t]{5.5cm}{\tt dlomax(alpha,sigma) \\ $\alpha>0, \sigma>0$ } & \parbox[c][1.5cm]{5.5cm}{$$  \frac{\alpha}{\sigma} \, \left(1+\frac{x}{\sigma}\right)^{-\left(\alpha+1\right)} $$} & $0$ \\
DuMouchel $^3$ & \parbox[t]{5.5cm}{\tt dmouch(sigma) \\ $\sigma>0$ } & \parbox[c][1.5cm]{5.5cm}{$$\frac{\sigma}{\left(x + \sigma\right)^2} $$} & $0$ \\
Gen. Par. & \parbox[t]{5.5cm}{\tt dgenpar(sigma,mu,xi) \\ $\sigma>0$ } & \parbox[c][1.5cm]{5.5cm}{$$\frac{1}{\sigma} \, \left(1+\xi \frac{x-\mu}\sigma\right)^{-\left(\frac{1}{\xi}+1\right)} $$} & $\mu$ $^4$ \\
 & & \parbox[c][1.5cm]{5.5cm}{For $\xi=0$:\;\; $\frac{1}{\sigma} \, e^{\frac{-\left(x-\mu\right)}{\sigma}} $} & $\mu$ \\
\hline

\end{tabular}

$^1$  This is equivalent to the {\tt dpar(alpha,c)} distribution and provided for naming consistency 

$^2$  This is referred to as the `2\textsuperscript{nd} kind Pareto' distribution by \citet{D2009};  an alternative form for the PDF of this distribution is given by:  $\frac{\alpha \, \sigma^\alpha}{\left(x + \sigma\right)^{\alpha+1}} $

$^3$  This distribution was suggested by \cite{DuMouchel1994} as a suitable prior for $\tau$ in a Bayesian meta-analysis setting, and is equivalent to a Lomax distribution with $\alpha=1$

$^4$  The Generalised Pareto distribution also has an upper bound of $x \le \mu - \frac{\sigma}{\xi}$ for $\xi < 0$

\caption{Distributions provided by the \proglang{JAGS} module included with \pkg{runjags}.}

\label{tab:pareto}
\end{table}

A shared library containing this module is installed within the \pkg{runjags} package, and will be loaded when either the `module' argument contains `runjags', or automatically when using any of the distributions or functions provided by the module.  Alternatively, the module can be loaded for use with the \pkg{rjags} package using the following command:

\begin{Schunk}
\begin{Sinput}
R> load.runjagsmodule()
\end{Sinput}
\begin{Soutput}
Loading required package: rjags
Loading required package: coda
Loading required package: lattice
Linked to JAGS 3.4.0
Loaded modules: basemod,bugs
module runjags loaded
\end{Soutput}
\end{Schunk}

Use of the internal module is only available for the `rjags' method (attempts to load the module will produce an error for other methods), however a standalone \proglang{JAGS} module containing the same functions is available from \url{http://sourceforge.net/projects/runjags}.  The configuration of this module is based on the template given by \cite{Wabersich}, and should therefore install on a variety of platforms using the standard `./configure', `make' `make install' convention.  Note that this module is named `runjagsmodule' to avoid naming conflicts with the internal module, and the internal module will not be loaded automatically if the character vector provided to the `module' argument contains `runjagsmodule'.


\section{Illustration of usage}
\label{sec:illustration}

Here we will consider a worked example of a simulation study analysis using \pkg{runjags}, to assess the sensitivity to various minimally informative priors of an over dispersed count model represented by multiple formulations of a negative binomial or gamma-Poisson compound distribution (for the sake of completeness, a proof of the equivalence of these distributions is included in Appendix~\ref{sec:derivation}).  Use of this distribution is widespread in many biological fields \citep{Bolker2009}, including parasitology \citep{Wilson199733,Wilson:1996vm,Shaw:1998ty}, where Bayesian methods of analysis have been shown to provide more robust inference than traditional methods \citep{Denwood:2008rz,Denwood:2010fk}.  A pragmatic assessment of the sensitivity of this distribution to subtly different parameterisations is therefore merited.


\subsection{Model formulation and assessment}
\label{sec:origmodel}

The gamma distribution is parameterised in \proglang{JAGS} and \proglang{BUGS} by the \jagscode{shape} ($\alpha$) and \jagscode{rate} ($\beta$) parameters, with the expectation given by $\frac{\alpha}{\beta}$ and variance given by $\frac{\alpha}{\beta^2}$.  This distribution can be used to describe underlying variability in a Poisson observation, representing an unknown amount of over-dispersion between observations.  In this situation the extra-Poisson coefficient of variation may be a more useful measure of the variability of the underlying gamma distribution \cite{denwood2010thesis};  this is represented by a function of the \jagscode{shape} parameter: $\sqrt{\frac{1}{\alpha}}$  

A candidate \proglang{JAGS} model (using inline data and monitor statements to be detected by \pkg{runjags}) is as follows:

\begin{Schunk}
\begin{Sinput}
R> jagsmodel <- "model{
+
+	for(i in 1:N){
+		Count[i] ~ dpois(lambda[i])
+		lambda[i] ~ dgamma(shape, rate)	
+	}
+	
+	shape ~ dgamma(0.001, 0.001)
+	mean ~ dgamma(0.001, 0.001)
+	rate <- shape / mean
+	
+	#data# N
+	#inits# shape, mean, .RNG.seed, .RNG.name
+}"
\end{Sinput}
\end{Schunk}

This model allows each observed \jagscode{Count} to follow a Poisson distribution with \jagscode{lambda} drawn from a gamma distribution with \jagscode{shape} parameter to be estimated, and \jagscode{rate} parameter calculated from the \jagscode{shape} parameter and the \jagscode{mean} of the distribution, which is also to be estimated.  The \jagscode{Gamma(0.001,0.001)} distribution is a commonly used `reference prior' for variance parameters such as the \jagscode{shape} parameter of our gamma distribution;  here we use the same minimally informative prior for both \jagscode{shape} and \jagscode{mean} parameters.  The \jagscode{\#data\#} statement is used to include \jagscode{N} as data that does not change between simulations, and \jagscode{\#inits\#} is used to include \jagscode{shape} and \jagscode{mean} as initial values for two chains.  The \jagscode{Count} variable is also observed, but will vary between simulations so is not retrieved from R memory using the \jagscode{\#data\#} tag.

The performance of this model can be assessed using a simulation study, with data generated from a gamma-Poisson distribution with a mean of 2, shape parameter of 0.75, and a sample size of 20.  These values are chosen to exaggerate any model performance issues by providing a comparatively small dataset with a large number of zero observations, and are similar to those typically found in veterinary parasitological datasets.  This assessment can be automated using the `run.jags.study' function, by creating a function to return some pre-generated simulated data, and then running the simulation study using a \pkg{snow} cluster of 20 processors (on the host machine) as follows.

Create simulated data and function to return one dataset per simulation:
\begin{Schunk}
\begin{Sinput}
R> N <- 20
R> S <- 1000
R> truemean <- 2
R> trueshape <- 0.75
R> truerate <- trueshape / truemean
R> set.seed(1)
R> alldata <- lapply(1:S, function(x){
+	return( rpois(N, rgamma(N, trueshape, rate = truerate) ) )
+ })
R> datafunction <- function(i)
+  return( list( Count = alldata[[i]] ) )
\end{Sinput}
\end{Schunk}
Set up initial values for 2 chains and a snow cluster (with 20 parallel threads on the local machine):
\begin{Schunk}
\begin{Sinput}
R> shape <- list(0.1, 10)
R> mean <- list(10, 0.1)
R> .RNG.seed <- function(chain) 
+  return( switch(chain, "1"= 1, "2"= 2) )
R> .RNG.name <- function(chain)
+  return( switch(chain, "1" = "base::Super-Duper",
+  "2" = "base::Wichmann-Hill") )
R> library("parallel")
R> cl <- makeCluster(20)
\end{Sinput}
\end{Schunk}
Run the simulation study and show the results:
\begin{Schunk}
\begin{Sinput}
R> results <- run.jags.study(S, jagsmodel, datafunction, 
+ targets = list( mean = truemean, shape = trueshape ), 
+ runjags.options = list( n.chains = 2 ), cl = cl)
\end{Sinput}
\begin{Soutput}
Starting a JAGS study at 01:17
Testing the model and data for simulation 990...
Compiling rjags model and adapting for 1000 iterations...
Finished running the simulation
The model runs OK
Calling autorun.jags for 1000 simulations...
Finished running the simulations
Finished summarising results
Finished JAGS study at 01:27 (total time taken: 6.9 minutes)
\end{Soutput}
\begin{Sinput}
R> results
\end{Sinput}
\begin{Soutput}
Average values obtained from a JAGS study with a total of 999 simulations
(excluding 1 crashed simulations):

      Target   Median     Mean Lower95%CI Upper95%CI Range95%CI Within95%CI
mean    2.00 2.037695 2.195683  0.9618164   3.732989   2.771172   0.9329329
shape   0.75 1.852515 6.025725  0.1719675  26.917416  26.745448   0.9579580
      AutoCorr(Lag10) Simulations
mean        0.1137136         999
shape       0.4883676         999

The 1 error returned has been stored in the '$errors' element of the list returned from run.jags.study

Average time taken:  5.2 seconds (range: 2.4 seconds - 36.9 seconds)
Average burnin required:  7272 (range: 5000 - 145000)
Average samples required:  10470 (range: 10000 - 57825)
\end{Soutput}
\end{Schunk}

The function returns an object of class `runjags.study', with a default print method that summarises the results as appropriate; showing average values for the parameters that are common to multiple simulations, and individual results for parameters that are estimated from only a single simulation (for drop-1 cross validation studies).  All individual simulations are run using the underlying autorun.jags function, which attempts to ensure that sufficient samples have been taken to ensure convergence and minimise Monte Carlo error.

For this simulation study, the \jagscode{mean} parameter was estimated reasonably well, but there was a large positive bias for the median and mean estimates of the \jagscode{shape} parameter which would result in under-estimation of the coefficient of variation.  As would be expected, the 95\% confidence intervals for both parameters identified the true value approximately 95\% of the time.  However, there was substantial autocorrelation for the shape parameter, which will have the effect of slowing convergence and increasing the required number of samples.  One of the simulations also returned an error; further investigation revealed that the cause of the error (`Slicer stuck at value with infinite density') was sampling of extremely small values for the \jagscode{shape} parameter for a dataset with a particularly large variance and small mean.


\subsection{Sensitivity to prior distributions}
\label{sec:priors}

There are various different minimally informative priors advocated for use with variance parameters in hierarchical models.  The commonly used \jagscode{Gamma(0.001,0.001)} distribution is characterised by a mean of 1 and a very large variance, and is almost improper \citep{Gelman2006}.  One alternative distribution that is guaranteed to be proper is a Uniform distribution with a lower bound of 0 and arbitrarily large upper bound, such as a \jagscode{Uniform(0,1000)} distribution.  For use with variance parameters in hierarchical models, \citet{DuMouchel1994} proposed the use of a prior distribution with PDF given by:

$$\pi\left(\tau\right) = \frac{s_0}{\left(s_0 + \tau\right)^2}$$

Although this connection is not stated directly by the author, the PDF given above is equivalent to that of a Lomax distribution with $\tau=x$, $s_0=\sigma$ and $\alpha=1$, and therefore to a Pareto type II distribution with $\tau=x$, $s_0=\sigma$,  $\alpha=1$ and $\mu=0$ (Table~\ref{tab:pareto}).  This DuMouchel distribution is guaranteed to be proper, and has a mode of zero with infinite mean and variance, which are conceptually desirable properties for a minimally informative prior.  The choice of $\sigma$ dictates the median of the distribution, with a value of 1 advocated because this also ensures invariance to the inverse transformation of $\tau$, meaning that this prior can be equivalently applied to the variance or precision.  If a different choice of $\sigma$ is made, then the distribution of $\frac{x}{\sigma}$ is invariant to inverse transformation.  Note that the Lomax (or Pareto Type II) distribution is not implemented directly in \proglang{JAGS} or \proglang{BUGS}, however the \pkg{runjags} module for \proglang{JAGS} implements this function as \jagscode{dlomax(alpha,sigma)}, and also provides the function \jagscode{dmouch(sigma)} that improves the computational efficiency of the underlying functions by removing the $\alpha$ parameter from the Lomax distribution.  Alternatively, a dummy variable distributed according to a Pareto(1,1) distribution can be used with the true parameter calculated by subtracting one from this dummy variable using standard \proglang{BUGS} syntax - however this is only equivalent to the Lomax distribution for the parameterisation with $\alpha=1, \sigma=1$.

As a result of the desirable properties mentioned above, this distribution has been used as a minimally informative prior in situations outside the meta-analysis application for which it was originally devised \citep[for example][]{Conti2011,Yin2013,Phillips2010}.  Here, a pragmatic approach is taken to quantify the effect of some commonly used prior distributions on the inference made using the model specified in Section~\ref{sec:origmodel}.  Using every combination of \jagscode{Gamma(0.001,0.001)}, \jagscode{Uniform(0,1000)} and \jagscode{Lomax(1,1)} distributions for the mean and shape parameters gives a total of nine candidate models.  Each were run with the same 1000 simulated datasets, using very similar \proglang{R} code to that for the first model.  

The results of these nine simulation studies are shown in Tables~\ref{tab:meanpriors} and~\ref{tab:shapepriors}.  Inference for the mean parameter was comparatively similar between the models, although use of a Uniform prior for the shape parameter resulted in poorly performing 95\% confidence intervals and higher autocorrelation for the mean parameter.  There was a marked difference between models in terms of the inference on the shape parameter, with a much larger bias and range of 95\% CI for models using a gamma and particularly Uniform prior on the shape parameter compared to models using the Lomax prior for the shape parameter.  The large bias of the Uniform prior is not surprising given that the mean of this distribution is 500, however a large positive bias was also observed with the Gamma prior with mean equal to 1.  Autocorrelation was much higher for the shape parameter than the mean parameter for all models, with the lowest autocorrelation (and all datasets analysed successfully) using Lomax priors for both parameters.

\begin{table}
\centering

\begin{tabular}{ l c c c c c c }

\hline

Mean Prior & Shape Prior & Mean & Range of CI & Within CI & Auto. Corr. & Simulations\\

\hline

Gamma & Gamma & 2.20 & 2.77 & 0.93 & 0.11 & 999\\
Uniform & Gamma & 2.67 & 4.24 & 0.95 & 0.24 & 996\\
Lomax & Gamma & 2.09 & 2.50 & 0.93 & 0.08 & 999\\
\hline
Gamma & Uniform & 2.13 & 2.31 & 0.86 & 0.27 & 1000\\
Uniform & Uniform & 2.39 & 2.90 & 0.89 & 0.34 & 999\\
Lomax & Uniform & 2.06 & 2.14 & 0.86 & 0.25 & 999\\
\hline
Gamma & Lomax & 2.18 & 2.70 & 0.93 & 0.07 & 999\\
Uniform & Lomax & 2.56 & 3.74 & 0.96 & 0.17 & 999\\
Lomax & Lomax & 2.09 & 2.47 & 0.93 & 0.04 & 1000\\

\hline

\end{tabular}
\caption{Average values for the inference on the mean parameter (true value 2) obtained from 1000 simulated datasets for nine gamma-Poisson MCMC models using three different minimally informative priors for the mean and shape parameters.}

\label{tab:meanpriors}
\end{table}


\begin{table}
\centering

\begin{tabular}{ l c c c c c c }

\hline

Mean Prior & Shape Prior & Mean & Range of CI & Within CI & Auto. Corr. & Simulations\\

\hline

Gamma & Gamma & 6.03 & 26.75 & 0.96 & 0.49 & 999\\
Uniform & Gamma & 5.86 & 25.36 & 0.95 & 0.49 & 996\\
Lomax & Gamma & 5.81 & 25.66 & 0.96 & 0.48 & 999\\
\hline
Gamma & Uniform & 115.42 & 255.49 & 0.92 & 0.59 & 1000\\
Uniform & Uniform & 112.49 & 250.81 & 0.93 & 0.60 & 999\\
Lomax & Uniform & 114.65 & 249.44 & 0.92 & 0.59 & 999\\
\hline
Gamma & Lomax & 1.47 & 3.99 & 0.97 & 0.37 & 999\\
Uniform & Lomax & 1.44 & 3.91 & 0.96 & 0.39 & 999\\
Lomax & Lomax & 1.49 & 4.04 & 0.97 & 0.38 & 1000\\

\hline

\end{tabular}

\caption{Average values for the inference on the shape parameter (true value 0.75) obtained from 1000 simulated datasets for nine gamma-Poisson MCMC models using three different minimally informative priors for the mean and shape parameters.}

\label{tab:shapepriors}
\end{table}


\subsection{Sensitivity of alternative model parameterisations}

The model shown in Section~\ref{sec:origmodel} (hereafter denoted `Model A') is not the only possible formulation of a gamma-Poisson distribution using the \proglang{BUGS} syntax.  In Model A, the prior distributions were placed on the \jagscode{mean} and \jagscode{shape} parameter with the \jagscode{rate} parameter calculated as a deterministic node from these, however it is equally possible to put the prior on the \jagscode{rate} parameter and calculate the \jagscode{mean} parameter deterministically (or even outside \proglang{JAGS}) as in Model B:

\begin{Schunk}
\begin{Sinput}
R> ModelB <- "model{
+	for(i in 1:N){
+		Count[i] ~ dpois(lambda[i])
+		lambda[i] ~ dgamma(shape, rate)	
+	}
+	
+	shape ~ dgamma(0.001, 0.001)
+	rate ~ dgamma(0.001, 0.001)
+	mean <- shape / rate
+	
+	#data# N
+	#inits# shape, rate, .RNG.seed, .RNG.name
+}"
\end{Sinput}
\end{Schunk}

In this simple model, we could also formulate the model as a negative binomial distribution rather than a gamma mixture of Poisson distributions - the negative binomial distribution is parameterised by a probability \jagscode{p} and a second parameter equivalent to the \jagscode{shape} parameter (see Appendix~\ref{sec:derivation} for more details).  An alternative model formulation could therefore be represented as in Model C:

\begin{Schunk}
\begin{Sinput}
R> ModelC <- "model{
+
+	for(i in 1:N){
+		Count[i] ~ dnegbin(prob[i], shape)
+		prob[i] <- shape / (shape + mean)
+	}
+	
+	shape ~ dgamma(0.001, 0.001)
+	mean ~ dgamma(0.001, 0.001)
+	
+	#data# N
+	#inits# shape, mean, .RNG.seed, .RNG.name
+}"
\end{Sinput}
\end{Schunk}

These models are all intended to represent the same simple data structure, and share similar `minimally informative' prior distributions on the two parameters of interest.  A comparison of the posterior coverage and autocorrelation between these models can be made, using gamma priors and Lomax priors for both parameters to assess the relative sensitivity of the three models to the priors.  This procedure is performed using the code given in Section~\ref{sec:origmodel} for these six candidate model formulations.

A comparison of the results from each model is shown in Tables~\ref{tab:meanmodels} and~\ref{tab:shapemodels}.  The two models A \& B encountered an error from one dataset using gamma priors, but all datasets were analysed successfully using Lomax priors.  Inference for the mean parameter was very similar between models A and C, but was somewhat different for Model B using the gamma prior.  As before, inference for the shape parameter was heavily affected by the choice of prior distributions, with the models using Lomax priors out-performing the corresponding models using gamma priors for each model type.  The choice of model formulation had a small effect on the inference made for the shape parameter using Lomax priors, particularly in terms of the autocorrelation and range of the 95'\% CI produced, but a much larger effect on the inference made using gamma priors.  

\begin{table}
\centering

\begin{tabular}{ l c c c c c c}

\hline
Model & Prior & Mean & Range of CI & Within CI & Auto. Corr. & Simulations\\

\hline

Model A & Lomax & 2.09 & 2.47 & 0.93 & 0.04 & 1000\\
Model A & Gamma & 2.20 & 2.77 & 0.93 & 0.11 & 999\\
Model B & Lomax & 2.06 & 2.33 & 0.92 & 0.01 & 1000\\
Model B & Gamma & 3.87 & 2.78 & 0.94 & 0.03 & 999\\
Model C & Lomax & 2.08 & 2.44 & 0.93 & 0.02 & 1000\\
Model C & Gamma & 2.19 & 2.74 & 0.93 & 0.05 & 1000\\

\hline

\end{tabular}

\caption{Average values for the inference on the mean parameter (true value 2) obtained from 1000 simulated datasets for three equivalent specifications of gamma-Poisson MCMC models, each using two sets of minimally informative priors for the mean and shape parameters.}

\label{tab:meanmodels}
\end{table}

\begin{table}
\centering

\begin{tabular}{ l c c c c c c }

\hline
Model & Prior & Mean & Range of CI & Within CI & Auto. Corr. & Simulations\\

\hline

Model A & Lomax & 1.49 & 4.04 & 0.97 & 0.38 & 1000\\
Model A & Gamma & 6.03 & 26.75 & 0.96 & 0.49 & 999\\
Model B & Lomax & 1.25 & 2.67 & 0.98 & 0.57 & 1000\\
Model B & Gamma & 2.51 & 8.60 & 0.96 & 0.65 & 999\\
Model C & Lomax & 1.53 & 4.05 & 0.97 & 0.20 & 1000\\
Model C & Gamma & 9.31 & 47.17 & 0.96 & 0.33 & 1000\\

\hline

\end{tabular}
\caption{Average values for the inference on the shape parameter (true value 0.75) obtained from 1000 simulated datasets for three equivalent specifications of gamma-Poisson MCMC models, each using two sets of minimally informative priors for the mean and shape parameters.}

\label{tab:shapemodels}
\end{table}


\subsection{Discussion}

The results presented here demonstrate the potential affect of multiple prior distributions, each of which could in some way be considered `minimally informative', on the inference made from analysis of small over-dispersed count datasets.  The Uniform prior would not typically be used for a variance parameter, but is sometimes used for mean parameters, and is useful to demonstrate an extreme example of prior distribution influence.  The results also show that a poor choice of prior distribution can also exaggerate differences in inference made by conceptually equivalent model formulations, and adversely affect the autocorrelation dependence within chains, reducing the effective sample size of a fixed length MCMC chain.  DuMouchel's prior distribution (equivalent to a Lomax distribution) produced the least bias and autocorrelation for the simulated data analysed here;  this may be partly a function of the specific characteristics of the data generated for this study, although a similar study using data generated with a mean of 1 and shape parameter of 10 produced similar results (data not shown).  The desirable properties of this distribution, in terms of invariance to sampling the variance or precision, infinite mean/variance with a mode of 0, and being guaranteed to remain proper, suggest that the potential usefulness of this prior should be investigated for other applications (particularly for precision parameters).  Although this distribution is not directly implemented in \proglang{JAGS}, the \pkg{runjags} package implements the full Pareto family of distributions using an extension module.  Alternatively, it is relatively straightforward (although computationally slower) to obtain this distribution from a type I $Pareto(1,1)$ distribution by subtracting one from a dummy variable.


\section{Summary}

Given the flexibility and ease of use of \proglang{BUGS} type software packages, the recent widespread adoption of these statistical tools to analyse data from a variety of disciplines is not surprising.  The extendability of \proglang{JAGS} with user-specified modules is a major advantage compared to other implementations of \proglang{BUGS}, as it allows any arbitrary function or distribution to be implemented directly within \proglang{JAGS}, replacing the need for the `ones' trick for using customised likelihood functions.  A very useful tutorial on writing and installing a standalone \proglang{JAGS} module is provided by \cite{Wabersich}, however it is substantially easier to configure and install a \proglang{JAGS} module as part of an \proglang{R} package using the \pkg{rjags} interface because many of the necessary environmental variables are set up by `R CMD INSTALL' during installation of the package, leaving only a `configure' file in the route directory and `Makevars' file, specifying the required include files, libraries and make objects, to be specified by the user.  The source code for the \pkg{runjags} package is freely available be used as a template for other users, and includes a brief overview of the process of writing a module within the README file.  

There are huge advantages to using MCMC, but also some potential disadvantages associated with failure to identify poor convergence and high Monte Carlo error, as well as sensitivity of the inference made to using subtly different model specifications and prior distributions.  The \pkg{runjags} package attempts to partially safeguard against some of these difficulties by calculating and automatically reporting convergence and sample length diagnostics every time a \proglang{JAGS} model is run, and providing a more user friendly way to access commonly used visual convergence diagnostics and summary statistics.  The methods used to call the underlying executable are generalised and flexible, and can be used to parallellise model runs with multiple chains.  A further attraction of this package is the facilitation of simulation studies, with analysis of an arbitrary number of replicate datasets fully automated and parallelised using a single function call.

There has been considerable debate in the literature concerning the relative merits of subjective and objective Bayesian inference (see for example \citet{Berger:1988uv,Berger2006,Lele2009}).  Where prior information is available and can be justified, there can be no doubt that use of this information will have a beneficial effect on the posterior distributions of all parameters of interest - however, there are many applied fields where use of such prior information is often considered to be unjustified by the mainstream community.  In these situations, it may be desirable to use a minimally informative prior, however there is always the potential for inference to be sensitive even to priors that are intended to be minimally informative, especially when dealing with small datasets.  For over-dispersed count data with properties similar to that generated here, it appears that the prior distribution suggested by \citet{DuMouchel1994} (a Lomax or Pareto Type-II distribution) is a suitable minimally informative prior, however the most appropriate prior for other applications is likely to be highly specific to that application.  Embarking on a full simulation study to validate the model formulation used can be computationally demanding, but the availability of ever-increasing computing power brings these procedures well within reach.  Where there is any doubt over the optimal model formulation for a particular application, a pragmatic and robust approach should therefore include some validation of the choice of model against simulated data.

\section*{Acknowledgements}

The author is grateful to Stefano Conti for useful discussions about the use of Pareto family distributions as priors, and to John Kruschke, Jan Gläscher, Richard Reeve, and all of the people mentioned in the CHANGELOG file for reporting bugs and making comments and suggestions that have greatly aided in development of the \pkg{runjags} package.


\begin{thebibliography}{33}
\newcommand{\enquote}[1]{``#1''}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi:\discretionary{}{}{}#1}\else
  \providecommand{\doi}{doi:\discretionary{}{}{}\begingroup
  \urlstyle{rm}\Url}\fi
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem[{Berger(2006)}]{Berger2006}
Berger J (2006).
\newblock \enquote{{The Case for Objective Bayesian Analysis}.}
\newblock \emph{Bayesian Analysis}, \textbf{1}(3), 385--402.

\bibitem[{Berger and Berry(1988)}]{Berger:1988uv}
Berger JO, Berry D (1988).
\newblock \enquote{{Statistical Analysis and the Illusion of Objectivity}.}
\newblock \emph{American Scientist}, \textbf{76}, 159--165.

\bibitem[{Bolker \emph{et~al.}(2009)Bolker, Brooks, Clark, Geange, Poulsen,
  Stevens, and White}]{Bolker2009}
Bolker BM, Brooks ME, Clark CJ, Geange SW, Poulsen JR, Stevens MHH, White JSS
  (2009).
\newblock \enquote{{Generalized Linear Mixed Models: a Practical Guide for
  Ecology and Evolution.}}
\newblock \emph{Trends in Ecology \& Evolution}, \textbf{24}(3), 127--35.
\newblock ISSN 0169-5347.
\newblock \urlprefix\url{http://dx.doi.org/10.1016/j.tree.2008.10.008}.

\bibitem[{Brooks and Gelman(1998)}]{Brooks1998}
Brooks SP, Gelman A (1998).
\newblock \enquote{{General Methods for Monitoring Convergence of Iterative
  Simulations}.}
\newblock \emph{Journal of Computational and Graphical Statistics},
  \textbf{7}(4), 434--455.

\bibitem[{Brooks and Roberts(1998)}]{Brooks:1998ee}
Brooks SP, Roberts GO (1998).
\newblock \enquote{{Assessing Convergence of Markov Chain Monte Carlo
  Algorithms}.}
\newblock \emph{Statistics and Computing}, \textbf{8}, 319--333.

\bibitem[{Conti \emph{et~al.}(2011)Conti, Presanis, van Veen, Xiridou,
  Donoghoe, {Rinder Stengaard}, and {De Angelis}}]{Conti2011}
Conti S, Presanis AM, van Veen MG, Xiridou M, Donoghoe MC, {Rinder Stengaard}
  A, {De Angelis} D (2011).
\newblock \enquote{{Modeling of the HIV Infection Epidemic in the Netherlands:
  a Multi-Parameter Evidence Synthesis Approach}.}
\newblock \emph{The Annals of Applied Statistics}, \textbf{5}(4), 2359--2384.
\newblock ISSN 1932-6157.
\newblock \urlprefix\url{http://dx.doi.org/10.1214/11-AOAS488}.

\bibitem[{Denwood([In Review])}]{Denwood:2014ee}
Denwood, MJ (In Review).
\newblock \enquote{{runjags: An \proglang{R} Package Providing Interface Utilities, 
Distributed Computing Methods and Additional Distributions For MCMC Models 
in \proglang{JAGS}}.}
\newblock \emph{Journal of Statistical Software}.

\bibitem[{Denwood(2010)}]{denwood2010thesis}
Denwood MJ (2010).
\newblock \emph{{A Quantitative Approach to Improving the Analysis of Faecal
  Worm egg Count Data}}.
\newblock Doctoral thesis, University of Glasgow.
\newblock \urlprefix\url{http://www.gla.ac.uk/media/media%5C_149338%5C_en.pdf}.

\bibitem[{Denwood \emph{et~al.}(2010)Denwood, Reid, Love, Nielsen, Matthews,
  McKendrick, and Innocent}]{Denwood:2010fk}
Denwood MJ, Reid SWJ, Love S, Nielsen MK, Matthews L, McKendrick IJ, Innocent
  GT (2010).
\newblock \enquote{{Comparison of Three Alternative Methods for Analysis of
  Equine Faecal egg Count Reduction Test Data}.}
\newblock \emph{Preventive Veterinary Medicine}, \textbf{93}(4), 316--23.
\newblock ISSN 1873-1716.
\newblock \urlprefix\url{http://dx.doi.org/10.1016/j.prevetmed.2009.11.009}.

\bibitem[{Denwood \emph{et~al.}(2008)Denwood, Stear, Matthews, Reid, Toft, and
  Innocent}]{Denwood:2008rz}
Denwood MJ, Stear MJ, Matthews L, Reid SWJ, Toft N, Innocent GT (2008).
\newblock \enquote{{The Distribution of the Pathogenic Nematode
  \textit{Nematodirus battus} in Lambs is Zero-Inflated}.}
\newblock \emph{Parasitology}, \textbf{135}(10), 1225--1235.
\newblock ISSN 1469-8161 (Electronic).
\newblock \urlprefix\url{http://dx.doi.org/10.1017/S0031182008004708}.

\bibitem[{DuMouchel(1994)}]{DuMouchel1994}
DuMouchel W (1994).
\newblock \enquote{{Hierarchical Bayes Linear Models for Meta-Analysis}.}
\newblock \emph{Technical Report~27}, {National Institute of Statistical
  Sciences}.
\newblock
  \urlprefix\url{http://www.niss.org/sites/default/files/pdfs/technicalreports/tr27.pdf}.

\bibitem[{Garthwaite \emph{et~al.}(2005)Garthwaite, Kadane, and
  O'Hagan}]{Garthwaite2005}
Garthwaite PH, Kadane JB, O'Hagan A (2005).
\newblock \enquote{{Statistical Methods for Eliciting Probability
  Distributions}.}
\newblock \emph{Journal of the American Statistical Association},
  \textbf{100}(470), 680--701.
\newblock ISSN 0162-1459.
\newblock \urlprefix\url{http://dx.doi.org/10.1198/016214505000000105}.

\bibitem[{Gelman(2006)}]{Gelman2006}
Gelman A (2006).
\newblock \enquote{{Prior Distributions for Variance Parameters in Hierarchical
  Models}.}
\newblock \emph{Bayesian Analysis}, \textbf{1}(3), 515--533.

\bibitem[{Gelman and Rubin(1992)}]{Gelman1992}
Gelman A, Rubin DB (1992).
\newblock \enquote{{Inference from Iterative Simulation Using Multiple
  Sequences}.}
\newblock \emph{Statistical Science}, \textbf{7}(4), 457--472.
\newblock \urlprefix\url{http://www.jstor.org/stable/2246093}.

\bibitem[{Gilks \emph{et~al.}(1998)Gilks, Richardson, and
  Spiegelhalter}]{Gilks:1998pt}
Gilks WR, Richardson S, Spiegelhalter DJ (1998).
\newblock \emph{{Markov Chain Monte Carlo in Practice}}.
\newblock Chapman and Hall, Boca Raton, Fla.
\newblock ISBN 0412055511.
\newblock
  \urlprefix\url{http://www.loc.gov/catdir/enhancements/fy0646/98033429-d.html}.

\bibitem[{Hastings(1970)}]{Hastings:1970bi}
Hastings WK (1970).
\newblock \enquote{{Monte Carlo Sampling Methods Using Markov Chains and Their
  Applications}.}
\newblock \emph{Biometrika}, \textbf{57}(1), 97--109.
\newblock \urlprefix\url{http://dx.doi.org/10.1093/biomet/57.1.97}.

\bibitem[{Kass \emph{et~al.}(1998)Kass, Carlin, Gelman, and Neal}]{Kass:1998fu}
Kass RE, Carlin BP, Gelman A, Neal RM (1998).
\newblock \enquote{{Markov Chain Monte Carlo in Practice: a Roundtable
  Discussion}.}
\newblock \emph{The American Statistician}, \textbf{52}(2), 93--100.

\bibitem[{Kass and Wasserman(1996)}]{Kass1996}
Kass RE, Wasserman L (1996).
\newblock \enquote{{The Selection of Prior Distributions by Formal Rules}.}
\newblock \emph{Journal of the American Statistical Association},
  \textbf{91}(435), pp. 1343--1370.
\newblock ISSN 01621459.
\newblock \urlprefix\url{http://www.jstor.org/stable/2291752}.

\bibitem[{Lele and Dennis(2009)}]{Lele2009}
Lele SR, Dennis B (2009).
\newblock \enquote{{Bayesian Methods for Hierarchical Models: are Ecologists
  Making a Faustian Bargain?}}
\newblock \emph{Ecological Applications : a Publication of the Ecological
  Society of America}, \textbf{19}(3), 581--4.
\newblock ISSN 1051-0761.
\newblock \urlprefix\url{http://www.ncbi.nlm.nih.gov/pubmed/19425420}.

\bibitem[{Lunn \emph{et~al.}(2000)Lunn, Thomas, Best, and
  Spiegelhalter}]{Lunn00}
Lunn DJ, Thomas A, Best N, Spiegelhalter D (2000).
\newblock \enquote{{\pkg{WinBUGS} - a Bayesian Modelling Framework: Concepts,
  Structure, and Extensibility}.}
\newblock \emph{Statistics and Computing}, \textbf{10}(4), 325--337.
\newblock ISSN 0960-3174.
\newblock \urlprefix\url{http://dx.doi.org/10.1023/A:1008929526011}.

\bibitem[{Phillips \emph{et~al.}(2010)Phillips, Tam, Conti, Rodrigues, Brown,
  Iturriza-Gomara, Gray, and Lopman}]{Phillips2010}
Phillips G, Tam CC, Conti S, Rodrigues LC, Brown D, Iturriza-Gomara M, Gray J,
  Lopman B (2010).
\newblock \enquote{{Community Incidence of Norovirus-Associated Infectious
  Intestinal Disease in England: Improved Estimates Using Viral Load for
  Norovirus Diagnosis.}}
\newblock \emph{American Journal of Epidemiology}, \textbf{171}(9), 1014--22.
\newblock ISSN 1476-6256.
\newblock \urlprefix\url{http://dx.doi.org/10.1093/aje/kwq021}.

\bibitem[{Plummer(2013{\natexlab{a}})}]{jags}
Plummer M (2013{\natexlab{a}}).
\newblock \emph{{\proglang{Just Another Gibbs Sampler} (\proglang{JAGS})
  Software, Version-3.4.0}}.
\newblock \urlprefix\url{http://mcmc-jags.sourceforge.net}.

\bibitem[{Plummer(2013{\natexlab{b}})}]{Plummer2013}
Plummer M (2013{\natexlab{b}}).
\newblock \emph{{\pkg{rjags}: Bayesian Graphical Models Using Mcmc}}.
\newblock \urlprefix\url{http://cran.r-project.org/package=rjags}.

\bibitem[{Plummer \emph{et~al.}(2006)Plummer, Best, Cowles, and
  Vines}]{Plummer2006}
Plummer M, Best N, Cowles K, Vines K (2006).
\newblock \enquote{{\pkg{CODA}: Convergence Diagnosis and Output Analysis for
  MCMC}.}
\newblock \emph{R News}, \textbf{6}(1), 7--11.
\newblock \urlprefix\url{http://cran.r-project.org/doc/Rnews/}.

\bibitem[{Polson and Scott(2011)}]{Polson2011}
Polson NG, Scott JG (2011).
\newblock \enquote{{On the Half-Cauchy Prior for a Global Scale Parameter}.}
\newblock \emph{Cornell University Library: arXiv.org}.
\newblock \urlprefix\url{http://arxiv.org/abs/1104.4937}.

\bibitem[{Shaw \emph{et~al.}(1998)Shaw, Grenfell, and Dobson}]{Shaw:1998ty}
Shaw DJ, Grenfell BT, Dobson AP (1998).
\newblock \enquote{{Patterns of Macroparasite Aggregation in Wildlife Host
  Populations.}}
\newblock \emph{Parasitology}, \textbf{117}, 597--610.
\newblock ISSN 0031-1820.

\bibitem[{Tierney \emph{et~al.}(2013)Tierney, Rossini, Li, and
  Sevcikova}]{Tierney2013}
Tierney L, Rossini AJ, Li N, Sevcikova H (2013).
\newblock \emph{{\pkg{snow}: Simple Network of Workstations}}.
\newblock \urlprefix\url{http://cran.r-project.org/package=snow}.

\bibitem[{Toft \emph{et~al.}(2007)Toft, Innocent, Gettinby, and
  Reid}]{Toft:2007vl}
Toft N, Innocent GT, Gettinby G, Reid SWJ (2007).
\newblock \enquote{{Assessing the Convergence of Markov Chain Monte Carlo
  Methods: an Example from Evaluation of Diagnostic Tests in Absence of a Gold
  Standard.}}
\newblock \emph{Preventive Veterinary Medicine}, \textbf{79}(2-4), 244--256.
\newblock ISSN 0167-5877.
\newblock \urlprefix\url{http://dx.doi.org/10.1016/j.prevetmed.2007.01.003}.

\bibitem[{Tuyl \emph{et~al.}(2008)Tuyl, Gerlach, and Mengersen}]{Tuyl2008}
Tuyl F, Gerlach R, Mengersen K (2008).
\newblock \enquote{{A Comparison of Bayes--Laplace, Jeffreys, and Other
  Priors}.}
\newblock \emph{The American Statistician}, \textbf{62}(1), 40--44.
\newblock ISSN 0003-1305.
\newblock \urlprefix\url{http://dx.doi.org/10.1198/000313008X267839}.

\bibitem[{{Van Hauwermeiren} and Vose(2009)}]{D2009}
{Van Hauwermeiren} M, Vose D (2009).
\newblock \emph{{A Compendium of Distributions}}.
\newblock Vose Software, Ghent, Belgium.
\newblock \urlprefix\url{http://www.vosesoftware.com/content/ebook.pdf}.

\bibitem[{Wabersich and Vandekerckhove(2013)}]{Wabersich}
Wabersich D, Vandekerckhove J (2013).
\newblock \enquote{{Extending \proglang{JAGS}: a Tutorial on Adding Custom
  Distributions to \proglang{JAGS} (With a Diffusion Model Example)}.}
\newblock \emph{Behavior Research Methods}.
\newblock
  \urlprefix\url{http://www.cidlab.com/prints/wabersich2013extending.pdf}.

\bibitem[{Wilson and Grenfell(1997)}]{Wilson199733}
Wilson K, Grenfell BT (1997).
\newblock \enquote{{Generalized Linear Modelling for Parasitologists}.}
\newblock \emph{Parasitology Today}, \textbf{13}(1), 33--38.
\newblock ISSN 0169-4758.
\newblock \urlprefix\url{http://dx.doi.org/10.1016/S0169-4758(96)40009-6}.

\bibitem[{Wilson \emph{et~al.}(1996)Wilson, Grenfell, and Shaw}]{Wilson:1996vm}
Wilson K, Grenfell BT, Shaw DJ (1996).
\newblock \enquote{{Analysis of Aggregated Parasite Distributions: a Comparison
  of Methods}.}
\newblock \emph{Functional Ecology}, \textbf{10}, 592--601.

\bibitem[{Yin \emph{et~al.}(2013)Yin, Conti, Desai, Stafford, Slater, Gill, and
  Simms}]{Yin2013}
Yin Z, Conti S, Desai S, Stafford M, Slater W, Gill ON, Simms I (2013).
\newblock \enquote{{The Geographic Relationship Between Sexual Health
  Deprivation and the Index of Multiple Deprivation 2010: a Comparison of two
  Indices.}}
\newblock \emph{Sexual Health}, \textbf{10}(2), 102--11.
\newblock ISSN 1448-5028.
\newblock \urlprefix\url{http://dx.doi.org/10.1071/SH12057}.

\end{thebibliography}


\appendix
\newpage

\section{Formulation of the negative binomial as a gamma-Poisson}
\label{sec:derivation}

The compound probability mass function of a Poisson distribution (with mean $\lambda$) integrated over a gamma distribution (with shape and scale parameters $\alpha$ and $\beta$ respectively) is given in Equation~\ref{eqn:nbgp1}.

\begin{align}
    f(x; \alpha, \beta) &= \int_0^\infty \frac{\lambda^x}{x!} e^{-\lambda} \; . \; \beta^\alpha \frac{1}{\Gamma(\alpha)} \lambda^{\alpha-1} e^{-\beta \lambda} \; \mathrm{d} \lambda
    \label{eqn:nbgp1}
\end{align}

Substituting $\alpha=r$ and $\beta=\frac{1-p}{p}$ into Equation~\ref{eqn:nbgp1} gives Equation~\ref{eqn:nbgp2a}, which can be re-written and simplified to Equation~\ref{eqn:nbgp2b}.

\begin{align}
f(x; r, p) &= \int_0^\infty \frac{\lambda^x}{x!} e^{-\lambda} \; . \; \left(\frac{1-p}{p}\right)^r \frac{1}{\Gamma(r)} \lambda^{r-1} e^{-\left(\frac{1-p}{p}\right)\lambda} \; \mathrm{d} \lambda 
    \label{eqn:nbgp2a}  \\
&=   \frac{\left(1-p\right)^r}{x! \; p^r \; \Gamma(r)} \; \int_0^\infty \lambda^{x+r-1} e^{-\lambda} e^{- \frac{\left(1-p\right)\lambda}{p}} \;\mathrm{d}\lambda \\
&=   \frac{\left(1-p\right)^r}{x! \; p^r \; \Gamma(r)} \; \int_0^\infty \lambda^{x+r-1} e^{- \frac{\lambda}{p}} \;\mathrm{d}\lambda 
    \label{eqn:nbgp2b}
\end{align}

Substituting the gamma function $\frac{\Gamma(b+1)}{a^{b+1}} = \int_0^\infty t^b e^{-at} \mathrm{d}t$ for $a=\frac{1}{p}$, $b=x+r-1$ and $t=\lambda$ into Equation~\ref{eqn:nbgp2b} gives Equation~\ref{eqn:nbgp3a}.

\begin{align}
f(x; r, p) &=   \frac{\left(1-p\right)^r}{x! \; p^r \; \Gamma(r)} \; \frac{\Gamma(x+r-1+1)}{\left(\frac{1}{p}\right)^{x+r-1+1}} 
    \label{eqn:nbgp3a} \\
&=   \frac{\left(1-p\right)^r}{x! \; p^r \; \Gamma(r)} \; \Gamma(x+r) \, p^{x+r} \\
&=   \frac{\Gamma(x+r)}{x!\;\Gamma(r)} \; p^x (1-p)^r
    \label{eqn:nbgp3b}
\end{align}

Equation~\ref{eqn:nbgp3b} is the probability mass function of the negative binomial distribution defining the number of successes $x$ before $r$ failures with a probability of success $p$, which is therefore exactly equivalent to a gamma-Poisson compound distribution with mean $\frac{\alpha}{\beta}=\frac{pr}{1-p}$ and shape parameter $\alpha=r$. 

\end{document}

